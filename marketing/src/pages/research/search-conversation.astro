---
import ResearchLayout from '../../layouts/ResearchLayout.astro';
import { getConceptBySlug } from '../../data/research';
const concept = getConceptBySlug('search-conversation')!;
---

<ResearchLayout concept={concept}>
  <!-- Demo -->
  <div slot="demo">
    <div class="sc-demo bg-void-deep border border-brass/15 rounded-sm overflow-hidden">
      <!-- Header -->
      <div class="flex items-center justify-between px-5 py-3 border-b border-brass/10 bg-steel/50">
        <div class="font-mono text-[10px] font-bold tracking-widest uppercase text-brass/60">Memory Conversation</div>
        <div class="font-mono text-[10px] text-chrome-dark/30">semantic search + synthesis</div>
      </div>

      <!-- Messages -->
      <div class="sc-messages px-5 py-5 space-y-4 max-h-[420px] overflow-y-auto" id="sc-messages">

        <!-- Human message 1 -->
        <div class="sc-msg human" data-msg="0">
          <div class="sc-msg-bubble human">
            <p class="m-0 font-serif text-[13px] text-chrome/90 leading-relaxed">What do I know about deploying to production?</p>
          </div>
          <div class="font-mono text-[9px] text-chrome-dark/20 mt-1 text-right">you</div>
        </div>

        <!-- System message 1 -->
        <div class="sc-msg system" data-msg="1">
          <div class="sc-msg-bubble system">
            <p class="m-0 font-serif text-[13px] text-chrome/80 leading-relaxed mb-3">Based on 7 relevant memories, here is what you know about production deployments:</p>

            <div class="sc-citation mb-3">
              <div class="flex items-center gap-2 mb-1">
                <span class="sc-confidence high">0.95</span>
                <span class="font-mono text-[10px] text-chrome-dark/40">shared scope</span>
              </div>
              <p class="m-0 font-serif text-[12px] text-chrome/70 leading-relaxed">Always run a <strong>dry-run deployment</strong> before pushing to production. This catches configuration errors, missing environment variables, and service dependency issues before they affect live traffic.</p>
            </div>

            <div class="sc-citation mb-3">
              <div class="flex items-center gap-2 mb-1">
                <span class="sc-confidence high">0.92</span>
                <span class="font-mono text-[10px] text-chrome-dark/40">shared scope</span>
              </div>
              <p class="m-0 font-serif text-[12px] text-chrome/70 leading-relaxed">Production Docker builds should use <code class="text-[10px] bg-brass/10 text-brass/70 px-1 rounded">--no-cache</code> to prevent stale layer artifacts. Pin base images to SHA digests rather than mutable tags.</p>
            </div>

            <div class="sc-citation mb-3">
              <div class="flex items-center gap-2 mb-1">
                <span class="sc-confidence high">0.91</span>
                <span class="font-mono text-[10px] text-chrome-dark/40">shared scope</span>
              </div>
              <p class="m-0 font-serif text-[12px] text-chrome/70 leading-relaxed">Maintain a <strong>blue-green deployment</strong> configuration for instant rollback. This is essential during traffic spikes when rollback speed determines incident duration.</p>
            </div>

            <div class="sc-synthesis mt-4 pt-3 border-t border-brass/8">
              <div class="font-mono text-[9px] font-bold tracking-widest uppercase text-brass/40 mb-2">Synthesis</div>
              <p class="m-0 font-serif text-[12px] text-chrome-dark/70 leading-relaxed">Two high-confidence learnings agree that dry-runs are critical. One lower-confidence memory <span class="sc-confidence low inline">0.41</span> suggests skipping dry-runs for hotfixes in exchange for requiring two merge request approvers. This is a meaningful tension -- speed vs. safety -- that has not been formally resolved.</p>
            </div>
          </div>
          <div class="font-mono text-[9px] text-chrome-dark/20 mt-1">deja</div>
        </div>

        <!-- Human message 2 -->
        <div class="sc-msg human" data-msg="2">
          <div class="sc-msg-bubble human">
            <p class="m-0 font-serif text-[13px] text-chrome/90 leading-relaxed">Should I resolve that contradiction?</p>
          </div>
          <div class="font-mono text-[9px] text-chrome-dark/20 mt-1 text-right">you</div>
        </div>

        <!-- System message 2 -->
        <div class="sc-msg system" data-msg="3">
          <div class="sc-msg-bubble system">
            <p class="m-0 font-serif text-[13px] text-chrome/80 leading-relaxed mb-3">The contradiction is between these two positions:</p>

            <div class="sc-compare grid grid-cols-2 gap-3 mb-3">
              <div class="sc-compare-side">
                <div class="font-mono text-[9px] text-brass/50 uppercase tracking-wider mb-1">Position A <span class="sc-confidence high">0.92</span></div>
                <p class="m-0 font-serif text-[11px] text-chrome/60 leading-relaxed">Always dry-run before production, no exceptions.</p>
              </div>
              <div class="sc-compare-side">
                <div class="font-mono text-[9px] text-chrome-dark/40 uppercase tracking-wider mb-1">Position B <span class="sc-confidence low">0.41</span></div>
                <p class="m-0 font-serif text-[11px] text-chrome/60 leading-relaxed">Skip dry-run for hotfixes; require two approvers instead.</p>
              </div>
            </div>

            <p class="m-0 font-serif text-[12px] text-chrome-dark/70 leading-relaxed mb-3">Three options:</p>

            <div class="space-y-2">
              <button class="sc-option-btn">
                <span class="font-mono text-[10px] text-brass/60">1.</span>
                <span class="font-serif text-[12px] text-chrome/70">Keep both -- the contradiction is informative</span>
              </button>
              <button class="sc-option-btn">
                <span class="font-mono text-[10px] text-brass/60">2.</span>
                <span class="font-serif text-[12px] text-chrome/70">Merge into: "Dry-run for standard deploys; two-approver override for hotfixes"</span>
              </button>
              <button class="sc-option-btn">
                <span class="font-mono text-[10px] text-brass/60">3.</span>
                <span class="font-serif text-[12px] text-chrome/70">Discard Position B -- low confidence, may be outdated</span>
              </button>
            </div>
          </div>
          <div class="font-mono text-[9px] text-chrome-dark/20 mt-1">deja</div>
        </div>

        <!-- Typing indicator (hidden by default) -->
        <div class="sc-msg system sc-typing hidden" id="sc-typing">
          <div class="sc-msg-bubble system">
            <div class="sc-typing-dots">
              <span></span><span></span><span></span>
            </div>
          </div>
        </div>
      </div>

      <!-- Input -->
      <div class="border-t border-brass/10 px-5 py-3 bg-steel/30">
        <div class="flex items-center gap-3">
          <input
            type="text"
            id="sc-input"
            placeholder="Ask your memory something..."
            class="flex-1 bg-void border border-brass/15 rounded-sm px-3 py-2 text-[13px] font-serif text-chrome placeholder:text-chrome-dark/25 focus:outline-none focus:border-brass/30 transition-colors"
          />
          <button id="sc-send" class="font-mono text-[10px] tracking-wider uppercase px-3 py-2 rounded-sm bg-brass/15 text-brass border border-brass/20 hover:bg-brass/25 transition-colors">
            Send
          </button>
        </div>
      </div>
    </div>
  </div>

  <!-- ToC -->
  <Fragment slot="toc">
    <a href="#the-experience" class="block py-1 text-[12px] font-serif text-chrome-dark/50 hover:text-brass transition-colors">The experience</a>
    <a href="#what-it-reveals" class="block py-1 text-[12px] font-serif text-chrome-dark/50 hover:text-brass transition-colors">What it reveals</a>
    <a href="#deja-connection" class="block py-1 text-[12px] font-serif text-chrome-dark/50 hover:text-brass transition-colors">Deja connection</a>
    <a href="#design-tensions" class="block py-1 text-[12px] font-serif text-chrome-dark/50 hover:text-brass transition-colors">Design tensions</a>
    <a href="#the-key-question" class="block py-1 text-[12px] font-serif text-chrome-dark/50 hover:text-brass transition-colors">The key question</a>
  </Fragment>

  <!-- Content -->
  <h2 id="the-experience">The experience</h2>

  <p>You do not type a query and get a ranked list. You ask a question and receive an answer. The difference sounds subtle but it changes everything. When you type "What do I know about deploying to production?" into the search conversation, you get back a synthesized response that draws from seven relevant memories, cites three of the most confident ones with scores, identifies a contradiction between two of them, and offers to help resolve it.</p>

  <p>This is not search. Search returns documents. The search conversation returns understanding. It reads across multiple memories, weighs their confidence, notices when two memories disagree, and presents you with a coherent narrative rather than a list of results ranked by similarity. The response has structure: high-confidence claims first, supporting details second, tensions and gaps last.</p>

  <p>The follow-up is where the interface becomes truly different. You can ask "Should I resolve that contradiction?" and the system presents the two conflicting positions side by side, with their confidence scores, and offers concrete options: keep both, merge them, or discard the weaker one. This is not retrieval anymore. This is collaborative reasoning about the shape of your own knowledge.</p>

  <p>The conversation feels like talking to a colleague who has perfect recall but no ego. They cite their sources. They flag what they are uncertain about. They do not pretend to know things they do not know. And they respond in natural language, which means you can ask vague questions ("What have I been learning about lately?") and get useful answers, unlike a keyword search that requires you to already know what you are looking for.</p>

  <h2 id="what-it-reveals">What it reveals</h2>

  <p>The search conversation reveals three things that other interfaces cannot: contradictions, gaps, and synthesis.</p>

  <p>Contradictions become visible because the system can compare the semantic content of multiple memories and notice when they disagree. A filing cabinet would show both memories as separate rows, and you would have to notice the conflict yourself. A journal would show them at different timestamps, with no indication that they are in tension. The search conversation brings them face-to-face: "Position A says always dry-run. Position B says skip for hotfixes. These disagree."</p>

  <p>Gaps become visible through absence. When you ask a question and the system can only find two relevant memories where you expected ten, the thinness of the response is itself information. The search conversation can also make this explicit: "I found 2 memories about monitoring but 0 about alerting thresholds. This might be a gap in your knowledge base." No other interface is positioned to deliver this kind of meta-observation.</p>

  <p>Synthesis is the core differentiator. The search conversation does not just find memories -- it reads them together and produces a response that is more than any individual memory. It can say things like "Across your deployment memories, the recurring theme is caution: dry-runs, blue-green setups, pinned images. You have a conservative deployment philosophy." This is an insight that exists nowhere in the raw data. It emerges only when the data is read together and interpreted.</p>

  <h2 id="deja-connection">How it connects to deja</h2>

  <p>The search conversation is deja's <code>/inject</code> endpoint elevated to a first-class interface. Today, <code>/inject</code> performs semantic search: it takes a context string, generates an embedding, queries the vector store (Cloudflare Vectorize with 384-dimension embeddings), and returns the top-k most similar memories. This is the retrieval layer. What is missing is the synthesis layer.</p>

  <p>The retrieval part works well. Deja already computes cosine similarity between the query embedding and stored memory embeddings. It already returns confidence scores. It already filters by scope. The hard part -- the vector search against a high-dimensional space -- is solved. What remains is wrapping that retrieval in a conversational interface that can synthesize multiple results into a coherent answer.</p>

  <p>The synthesis layer requires a language model. The search conversation would call <code>/inject</code> to get relevant memories, then pass those memories as context to an LLM with a prompt like: "Given these memories, answer the user's question. Cite specific memories with their confidence scores. Note any contradictions. Identify gaps." The LLM does the reading-across that makes the interface powerful.</p>

  <p>This raises an architectural question: should this synthesis happen inside deja or outside it? Deja today is a memory layer, not a chat application. Adding LLM-powered synthesis would change its scope. The alternative is to build the search conversation as a separate application that calls deja's API for retrieval and its own LLM for synthesis. Both approaches are viable. The choice depends on whether deja wants to be a primitive (an API others build on) or a product (an experience users interact with directly).</p>

  <h2 id="design-tensions">Design tensions</h2>

  <p>The deepest design tension is authorship. When the search conversation synthesizes an answer, who is speaking? The memories were stored by an agent. The synthesis was generated by a different model. The user asked the question. The answer feels authoritative, but it is an interpretation -- a model's reading of stored data, not the data itself. If the synthesis is wrong, the user might blame the memories when the real issue is the synthesis layer.</p>

  <p>Citation is one mitigation. By showing confidence scores and linking to specific memories, the search conversation makes its reasoning transparent. But transparency is not the same as accuracy. A confident citation of a wrong memory is worse than no citation at all, because it creates a false sense of verification. The interface needs a way to communicate not just "I found this" but "I am uncertain about my reading of this."</p>

  <p>There is also a latency problem. Retrieval from a vector store is fast -- tens of milliseconds. LLM synthesis is slow -- seconds at minimum. The search conversation needs to feel responsive even though its most important computation takes orders of magnitude longer than a database query. Streaming responses help (the answer appears word by word), but there is still a perceptual gap between the instant filtering of a filing cabinet and the deliberate pace of a conversational response.</p>

  <p>Finally, the conversation creates an expectation of dialogue. Once users are talking to their memory, they will want to do more than ask questions. They will want to say "update that memory" or "merge these two" or "create a new memory based on this conversation." Each of these actions requires the search conversation to become a write interface, not just a read interface. And write operations through natural language are harder to make predictable and reversible than write operations through buttons and forms.</p>

  <h2 id="the-key-question">The key question</h2>

  <p>Where does the intelligence boundary lie?</p>

  <p>Deja is a memory layer. It stores, retrieves, and manages knowledge. The search conversation adds a synthesis layer on top -- an LLM that reads across memories and generates coherent answers. But this synthesis layer is exactly what agents already do. When a coding agent receives injected memories and uses them to inform its response, it is performing the same synthesis the search conversation would perform. The agent reads the memories, weighs them, notices contradictions, and acts.</p>

  <p>So is the search conversation deja's job or the agent's job? If deja builds synthesis into its own interface, it starts competing with its consumers. An agent using deja might get pre-synthesized answers when it wanted raw memories. A human using deja might get a conversational response when they wanted to see the underlying data.</p>

  <p>The answer may be that both modes need to exist. The API returns raw memories (the primitive). The search conversation returns synthesized answers (the product). One serves agents who do their own reasoning. The other serves humans who want understanding without doing the reasoning themselves. The key is that both are built on the same retrieval layer, and the synthesis layer is strictly additive -- never replacing the raw data, only augmenting it.</p>

  <p>This is the pattern that separates memory infrastructure from memory interfaces. The infrastructure is the vector store, the embeddings, the scope model, the confidence scores. The interfaces are the filing cabinet, the journal, the search conversation, the constellation map. Each interface reveals a different dimension of the same underlying data. The search conversation reveals the dimension of meaning -- what the memories say when read together. Whether that reading happens inside deja or outside it is an implementation detail. That it needs to happen is not.</p>
</ResearchLayout>

<style>
  .sc-demo {
    font-family: var(--font-serif), serif;
  }

  .sc-messages {
    scrollbar-width: thin;
    scrollbar-color: rgba(201, 169, 97, 0.15) transparent;
  }

  .sc-messages::-webkit-scrollbar {
    width: 4px;
  }

  .sc-messages::-webkit-scrollbar-track {
    background: transparent;
  }

  .sc-messages::-webkit-scrollbar-thumb {
    background: rgba(201, 169, 97, 0.15);
    border-radius: 2px;
  }

  .sc-msg {
    opacity: 0;
    transform: translateY(8px);
    animation: sc-msg-enter 0.4s ease-out forwards;
  }

  .sc-msg[data-msg="0"] { animation-delay: 0.2s; }
  .sc-msg[data-msg="1"] { animation-delay: 0.6s; }
  .sc-msg[data-msg="2"] { animation-delay: 1.2s; }
  .sc-msg[data-msg="3"] { animation-delay: 1.6s; }

  @keyframes sc-msg-enter {
    to {
      opacity: 1;
      transform: translateY(0);
    }
  }

  .sc-msg.human {
    display: flex;
    flex-direction: column;
    align-items: flex-end;
  }

  .sc-msg.system {
    display: flex;
    flex-direction: column;
    align-items: flex-start;
  }

  .sc-msg-bubble {
    max-width: 85%;
    border-radius: 6px;
    padding: 12px 16px;
  }

  .sc-msg-bubble.human {
    background: rgba(201, 169, 97, 0.1);
    border: 1px solid rgba(201, 169, 97, 0.15);
  }

  .sc-msg-bubble.system {
    background: rgba(26, 26, 26, 0.6);
    border: 1px solid rgba(201, 169, 97, 0.06);
  }

  .sc-confidence {
    font-family: var(--font-mono), monospace;
    font-size: 10px;
    font-weight: 600;
    padding: 1px 5px;
    border-radius: 2px;
  }

  .sc-confidence.high {
    color: #c9a961;
    background: rgba(201, 169, 97, 0.12);
  }

  .sc-confidence.low {
    color: rgba(184, 184, 184, 0.5);
    background: rgba(184, 184, 184, 0.08);
  }

  .sc-confidence.inline {
    display: inline;
    vertical-align: baseline;
  }

  .sc-citation {
    padding-left: 10px;
    border-left: 2px solid rgba(201, 169, 97, 0.15);
  }

  .sc-compare-side {
    background: rgba(10, 10, 10, 0.4);
    border: 1px solid rgba(201, 169, 97, 0.06);
    border-radius: 3px;
    padding: 8px 10px;
  }

  .sc-option-btn {
    display: flex;
    align-items: baseline;
    gap: 8px;
    width: 100%;
    text-align: left;
    background: rgba(10, 10, 10, 0.3);
    border: 1px solid rgba(201, 169, 97, 0.08);
    border-radius: 3px;
    padding: 8px 10px;
    cursor: pointer;
    transition: border-color 0.15s ease, background-color 0.15s ease;
  }

  .sc-option-btn:hover {
    border-color: rgba(201, 169, 97, 0.2);
    background: rgba(201, 169, 97, 0.05);
  }

  .sc-option-btn.selected {
    border-color: rgba(201, 169, 97, 0.3);
    background: rgba(201, 169, 97, 0.08);
  }

  .sc-typing-dots {
    display: flex;
    gap: 4px;
    padding: 4px 0;
  }

  .sc-typing-dots span {
    display: block;
    width: 6px;
    height: 6px;
    border-radius: 50%;
    background: rgba(201, 169, 97, 0.4);
    animation: sc-typing-bounce 1.2s ease-in-out infinite;
  }

  .sc-typing-dots span:nth-child(2) {
    animation-delay: 0.2s;
  }

  .sc-typing-dots span:nth-child(3) {
    animation-delay: 0.4s;
  }

  @keyframes sc-typing-bounce {
    0%, 60%, 100% { transform: translateY(0); opacity: 0.4; }
    30% { transform: translateY(-4px); opacity: 1; }
  }
</style>

<script>
  // Option button selection
  document.querySelectorAll('.sc-option-btn').forEach(btn => {
    btn.addEventListener('click', () => {
      document.querySelectorAll('.sc-option-btn').forEach(b => b.classList.remove('selected'));
      btn.classList.add('selected');
    });
  });

  // Input handling
  const input = document.getElementById('sc-input') as HTMLInputElement;
  const sendBtn = document.getElementById('sc-send') as HTMLButtonElement;
  const messages = document.getElementById('sc-messages') as HTMLDivElement;
  const typing = document.getElementById('sc-typing') as HTMLDivElement;

  const responses: Record<string, string> = {
    default: "I found 3 potentially relevant memories, but none with confidence above 0.7 for this specific question. The closest match relates to CI pipeline configuration. Would you like me to broaden the search across all scopes?",
    monitoring: "You have 4 memories about monitoring. The strongest pattern: always alert on p99 latency, not averages. One memory suggests setting up dashboards before deploying new services. Another notes that monitoring gaps are most dangerous during the first 48 hours after a deployment.",
    testing: "Your testing knowledge clusters around two themes: parallelization (run test suites by module to cut CI time) and isolation (never share database state between test runs). Confidence is moderate -- 0.66 to 0.79 -- suggesting these are practices you have tried but not fully validated.",
  };

  function addMessage(text: string, isHuman: boolean) {
    const msg = document.createElement('div');
    msg.className = `sc-msg ${isHuman ? 'human' : 'system'}`;

    const bubble = document.createElement('div');
    bubble.className = `sc-msg-bubble ${isHuman ? 'human' : 'system'}`;

    const p = document.createElement('p');
    p.className = 'm-0 font-serif text-[13px] leading-relaxed';
    p.classList.add(isHuman ? 'text-chrome/90' : 'text-chrome/80');
    p.textContent = text;

    bubble.appendChild(p);
    msg.appendChild(bubble);

    const label = document.createElement('div');
    label.className = 'font-mono text-[9px] text-chrome-dark/20 mt-1';
    if (isHuman) label.classList.add('text-right');
    label.textContent = isHuman ? 'you' : 'deja';
    msg.appendChild(label);

    // Insert before typing indicator
    messages.insertBefore(msg, typing);

    // Animate
    requestAnimationFrame(() => {
      msg.style.opacity = '0';
      msg.style.transform = 'translateY(8px)';
      msg.style.transition = 'opacity 0.3s ease-out, transform 0.3s ease-out';
      requestAnimationFrame(() => {
        msg.style.opacity = '1';
        msg.style.transform = 'translateY(0)';
      });
    });

    messages.scrollTop = messages.scrollHeight;
  }

  function getResponse(text: string): string {
    const lower = text.toLowerCase();
    if (lower.includes('monitor') || lower.includes('alert') || lower.includes('observ')) {
      return responses.monitoring;
    }
    if (lower.includes('test') || lower.includes('ci') || lower.includes('pipeline')) {
      return responses.testing;
    }
    return responses.default;
  }

  function handleSend() {
    const text = input.value.trim();
    if (!text) return;

    addMessage(text, true);
    input.value = '';

    // Show typing
    typing.classList.remove('hidden');
    messages.scrollTop = messages.scrollHeight;

    // Simulate response delay
    setTimeout(() => {
      typing.classList.add('hidden');
      addMessage(getResponse(text), false);
    }, 1200 + Math.random() * 800);
  }

  sendBtn.addEventListener('click', handleSend);
  input.addEventListener('keydown', (e) => {
    if (e.key === 'Enter') handleSend();
  });
</script>
